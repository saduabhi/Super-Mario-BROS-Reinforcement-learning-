{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5361184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (52.0.0.post20210125)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.20.1)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.36.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.0.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (1.26.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=96954fb6889b5f0c0a8d5534632560db13a4c00cd90bbfbe55647412c13e4a7f\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-gpu\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.1 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-gpu-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27dbc7",
   "metadata": {},
   "source": [
    "**Installing the game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73aa6aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym_super_mario_bros==7.3.0\n",
      "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
      "Collecting nes_py\n",
      "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
      "Collecting gym>=0.17.2\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (1.20.1)\n",
      "Collecting pyglet<=1.5.11,>=1.4.0\n",
      "  Downloading pyglet-1.5.11-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes_py) (4.59.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes_py) (4.11.3)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.10.0->gym>=0.17.2->nes_py) (3.4.1)\n",
      "Building wheels for collected packages: nes-py, gym\n",
      "  Building wheel for nes-py (setup.py): started\n",
      "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
      "  Created wheel for nes-py: filename=nes_py-8.1.8-cp38-cp38-win_amd64.whl size=47358 sha256=c2f79b0782a55b1c8aa891bb05eddf3235d1a2884b3d6f699487e4b3622e84c3\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\8d\\6e\\f0\\113c979eba40def28ee9b3c81a4adec00386106d81fb3bc2c2\n",
      "  Building wheel for gym (PEP 517): started\n",
      "  Building wheel for gym (PEP 517): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701358 sha256=4e1394eacdf2e452a5fa10d15293eeb3630b11d5a1325f68359abe07a2c6fdde\n",
      "  Stored in directory: c:\\users\\computing\\appdata\\local\\pip\\cache\\wheels\\78\\28\\77\\b0c74e80a2a4faae0161d5c53bc4f8e436e77aedc79136ee13\n",
      "Successfully built nes-py gym\n",
      "Installing collected packages: gym-notices, pyglet, gym, nes-py, gym-super-mario-bros\n",
      "Successfully installed gym-0.23.1 gym-notices-0.0.6 gym-super-mario-bros-7.3.0 nes-py-8.1.8 pyglet-1.5.11\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_super_mario_bros==7.3.0 nes_py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c23fdf",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbe586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the game\n",
    "import gym_super_mario_bros\n",
    "# Importing the Joypad wrapper\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Importing the SIMPLIFIED controls\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78a3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c352577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408beea3",
   "metadata": {},
   "source": [
    "**Grayscaling for observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14813e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cff57b",
   "metadata": {},
   "source": [
    "**Resizing the observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcfff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-1-1-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env.seed(32)\n",
    "env.action_space.seed(32)\n",
    "torch.manual_seed(32)\n",
    "torch.random.manual_seed(32)\n",
    "np.random.seed(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0882fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df6079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSolver:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.95\n",
    "        self.lamda = 0.95\n",
    "        self.worker_steps = 4096\n",
    "        self.n_mini_batch = 4\n",
    "        self.epochs = 30\n",
    "        self.save_directory = r\"C:\\Users\\Computing\\Desktop\\PPO without baseline model\"\n",
    "        self.batch_size = self.worker_steps\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.obs = env.reset().__array__()\n",
    "        self.policy = Model().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = Model().to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.all_episode_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "\n",
    "    def sample(self):\n",
    "        rewards = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        actions = np.zeros(self.worker_steps, dtype=np.int32)\n",
    "        done = np.zeros(self.worker_steps, dtype=bool)\n",
    "        obs = np.zeros((self.worker_steps, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        values = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        for t in range(self.worker_steps):\n",
    "            with torch.no_grad():\n",
    "                obs[t] = self.obs\n",
    "                pi, v = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                values[t] = v.cpu().numpy()\n",
    "                a = pi.sample()\n",
    "                actions[t] = a.cpu().numpy()\n",
    "                log_pis[t] = pi.log_prob(a).cpu().numpy()\n",
    "            self.obs, rewards[t], done[t], _ = env.step(actions[t])\n",
    "            self.obs = self.obs.__array__()\n",
    "            env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                self.episode += 1\n",
    "                self.all_episode_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.all_episode_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.all_episode_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_checkpoint()\n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        return {\n",
    "            'obs': torch.tensor(obs.reshape(obs.shape[0], *obs.shape[1:]), dtype=torch.float32, device=device),\n",
    "            'actions': torch.tensor(actions, device=device),\n",
    "            'values': torch.tensor(values, device=device),\n",
    "            'log_pis': torch.tensor(log_pis, device=device),\n",
    "            'advantages': torch.tensor(advantages, device=device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.cpu().data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        pi, value = self.policy(samples['obs'])\n",
    "        ratio = torch.exp(pi.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = pi.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f133e",
   "metadata": {},
   "source": [
    "**Using PPO agent as solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05317a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average reward: 575.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_10.pth'\n",
      "Episode: 20, average reward: 661.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_20.pth'\n",
      "Episode: 30, average reward: 748.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_30.pth'\n",
      "Episode: 40, average reward: 737.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_40.pth'\n",
      "Episode: 50, average reward: 482.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_50.pth'\n",
      "Episode: 60, average reward: 617.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_60.pth'\n",
      "Episode: 70, average reward: 872.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_70.pth'\n",
      "Episode: 80, average reward: 594.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_80.pth'\n",
      "Episode: 90, average reward: 648.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_90.pth'\n",
      "Episode: 100, average reward: 690.7000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_100.pth'\n",
      "Episode: 110, average reward: 747.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_110.pth'\n",
      "Episode: 120, average reward: 890.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_120.pth'\n",
      "Episode: 130, average reward: 487.29998779296875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_130.pth'\n",
      "Episode: 140, average reward: 722.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_140.pth'\n",
      "Episode: 150, average reward: 818.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_150.pth'\n",
      "Episode: 160, average reward: 867.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_160.pth'\n",
      "Episode: 170, average reward: 885.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_170.pth'\n",
      "Episode: 180, average reward: 545.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_180.pth'\n",
      "Episode: 190, average reward: 794.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_190.pth'\n",
      "Episode: 200, average reward: 725.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_200.pth'\n",
      "Episode: 210, average reward: 679.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_210.pth'\n",
      "Episode: 220, average reward: 478.8999938964844\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_220.pth'\n",
      "Episode: 230, average reward: 599.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_230.pth'\n",
      "Episode: 240, average reward: 688.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_240.pth'\n",
      "Episode: 250, average reward: 516.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_250.pth'\n",
      "Episode: 260, average reward: 643.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_260.pth'\n",
      "Episode: 270, average reward: 503.79998779296875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_270.pth'\n",
      "Episode: 280, average reward: 706.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_280.pth'\n",
      "Episode: 290, average reward: 730.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_290.pth'\n",
      "Episode: 300, average reward: 536.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_300.pth'\n",
      "Episode: 310, average reward: 578.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_310.pth'\n",
      "Episode: 320, average reward: 717.7000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_320.pth'\n",
      "Episode: 330, average reward: 607.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_330.pth'\n",
      "Episode: 340, average reward: 542.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_340.pth'\n",
      "Episode: 350, average reward: 501.6000061035156\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_350.pth'\n",
      "Episode: 360, average reward: 905.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_360.pth'\n",
      "Episode: 370, average reward: 625.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_370.pth'\n",
      "Episode: 380, average reward: 430.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_380.pth'\n",
      "Episode: 390, average reward: 573.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_390.pth'\n",
      "Episode: 400, average reward: 759.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_400.pth'\n",
      "Episode: 410, average reward: 716.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_410.pth'\n",
      "Episode: 420, average reward: 699.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_420.pth'\n",
      "Episode: 430, average reward: 591.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_430.pth'\n",
      "Episode: 440, average reward: 726.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_440.pth'\n",
      "Episode: 450, average reward: 585.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_450.pth'\n",
      "Episode: 460, average reward: 635.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_460.pth'\n",
      "Episode: 470, average reward: 660.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_470.pth'\n",
      "Episode: 480, average reward: 720.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_480.pth'\n",
      "Episode: 490, average reward: 595.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_490.pth'\n",
      "Episode: 500, average reward: 902.7000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_500.pth'\n",
      "Episode: 510, average reward: 709.7000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_510.pth'\n",
      "Episode: 520, average reward: 1051.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_520.pth'\n",
      "Episode: 530, average reward: 794.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_530.pth'\n",
      "Episode: 540, average reward: 772.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_540.pth'\n",
      "Episode: 550, average reward: 898.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_550.pth'\n",
      "Episode: 560, average reward: 711.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_560.pth'\n",
      "Episode: 570, average reward: 769.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_570.pth'\n",
      "Episode: 580, average reward: 665.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_580.pth'\n",
      "Episode: 590, average reward: 852.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_590.pth'\n",
      "Episode: 600, average reward: 597.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_600.pth'\n",
      "Episode: 610, average reward: 676.7000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_610.pth'\n",
      "Episode: 620, average reward: 893.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_620.pth'\n",
      "Episode: 630, average reward: 1058.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_630.pth'\n",
      "Episode: 640, average reward: 770.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_640.pth'\n",
      "Episode: 650, average reward: 875.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_650.pth'\n",
      "Episode: 660, average reward: 621.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_660.pth'\n",
      "Episode: 670, average reward: 965.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_670.pth'\n",
      "Episode: 680, average reward: 806.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_680.pth'\n",
      "Episode: 690, average reward: 742.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_690.pth'\n",
      "Episode: 700, average reward: 859.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_700.pth'\n",
      "Episode: 710, average reward: 744.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_710.pth'\n",
      "Episode: 720, average reward: 780.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_720.pth'\n",
      "Episode: 730, average reward: 786.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_730.pth'\n",
      "Episode: 740, average reward: 961.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_740.pth'\n",
      "Episode: 750, average reward: 867.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_750.pth'\n",
      "Episode: 760, average reward: 959.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_760.pth'\n",
      "Episode: 770, average reward: 929.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_770.pth'\n",
      "Episode: 780, average reward: 986.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_780.pth'\n",
      "Episode: 790, average reward: 936.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_790.pth'\n",
      "Episode: 800, average reward: 886.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_800.pth'\n",
      "Episode: 810, average reward: 974.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_810.pth'\n",
      "Episode: 820, average reward: 1035.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_820.pth'\n",
      "Episode: 830, average reward: 965.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_830.pth'\n",
      "Episode: 840, average reward: 835.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_840.pth'\n",
      "Episode: 850, average reward: 719.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_850.pth'\n",
      "Episode: 860, average reward: 972.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_860.pth'\n",
      "Episode: 870, average reward: 883.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_870.pth'\n",
      "Episode: 880, average reward: 752.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_880.pth'\n",
      "Episode: 890, average reward: 983.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_890.pth'\n",
      "Episode: 900, average reward: 732.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_900.pth'\n",
      "Episode: 910, average reward: 772.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_910.pth'\n",
      "Episode: 920, average reward: 917.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_920.pth'\n",
      "Episode: 930, average reward: 957.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_930.pth'\n",
      "Episode: 940, average reward: 849.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_940.pth'\n",
      "Episode: 950, average reward: 842.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_950.pth'\n",
      "Episode: 960, average reward: 859.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_960.pth'\n",
      "Episode: 970, average reward: 997.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_970.pth'\n",
      "Episode: 980, average reward: 922.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_980.pth'\n",
      "Episode: 990, average reward: 893.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_990.pth'\n",
      "Episode: 1000, average reward: 1045.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1000.pth'\n",
      "Episode: 1010, average reward: 852.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1010.pth'\n",
      "Episode: 1020, average reward: 967.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1020.pth'\n",
      "Episode: 1030, average reward: 975.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1030.pth'\n",
      "Episode: 1040, average reward: 998.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1040.pth'\n",
      "Episode: 1050, average reward: 959.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1050.pth'\n",
      "Episode: 1060, average reward: 942.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1060.pth'\n",
      "Episode: 1070, average reward: 966.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1070.pth'\n",
      "Episode: 1080, average reward: 951.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1080.pth'\n",
      "Episode: 1090, average reward: 1091.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1090.pth'\n",
      "Episode: 1100, average reward: 1151.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1100.pth'\n",
      "Episode: 1110, average reward: 975.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1110.pth'\n",
      "Episode: 1120, average reward: 1048.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1120.pth'\n",
      "Episode: 1130, average reward: 803.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1130.pth'\n",
      "Episode: 1140, average reward: 966.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1140.pth'\n",
      "Episode: 1150, average reward: 953.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1150.pth'\n",
      "Episode: 1160, average reward: 691.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1160.pth'\n",
      "Episode: 1170, average reward: 928.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1170.pth'\n",
      "Episode: 1180, average reward: 817.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1180.pth'\n",
      "Episode: 1190, average reward: 998.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1190.pth'\n",
      "Episode: 1200, average reward: 960.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1200.pth'\n",
      "Episode: 1210, average reward: 1068.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1210.pth'\n",
      "Episode: 1220, average reward: 945.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1220.pth'\n",
      "Episode: 1230, average reward: 753.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1230.pth'\n",
      "Episode: 1240, average reward: 968.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1240.pth'\n",
      "Episode: 1250, average reward: 908.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1250.pth'\n",
      "Episode: 1260, average reward: 1116.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1260.pth'\n",
      "Episode: 1270, average reward: 969.7999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1270.pth'\n",
      "Episode: 1280, average reward: 1124.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1280.pth'\n",
      "Episode: 1290, average reward: 1062.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1290.pth'\n",
      "Episode: 1300, average reward: 1130.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1300.pth'\n",
      "Episode: 1310, average reward: 1122.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1310.pth'\n",
      "Episode: 1320, average reward: 1086.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1320.pth'\n",
      "Episode: 1330, average reward: 1350.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1330.pth'\n",
      "Episode: 1340, average reward: 1042.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1340.pth'\n",
      "Episode: 1350, average reward: 1172.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1350.pth'\n",
      "Episode: 1360, average reward: 1083.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1360.pth'\n",
      "Episode: 1370, average reward: 1058.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1370.pth'\n",
      "Episode: 1380, average reward: 1300.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1380.pth'\n",
      "Episode: 1390, average reward: 1086.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1390.pth'\n",
      "Episode: 1400, average reward: 1204.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1400.pth'\n",
      "Episode: 1410, average reward: 1274.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1410.pth'\n",
      "Episode: 1420, average reward: 1135.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1420.pth'\n",
      "Episode: 1430, average reward: 1025.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1430.pth'\n",
      "Episode: 1440, average reward: 1116.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1440.pth'\n",
      "Episode: 1450, average reward: 1077.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1450.pth'\n",
      "Episode: 1460, average reward: 1278.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1460.pth'\n",
      "Episode: 1470, average reward: 1532.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1470.pth'\n",
      "Episode: 1480, average reward: 1150.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1480.pth'\n",
      "Episode: 1490, average reward: 1468.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1490.pth'\n",
      "Episode: 1500, average reward: 1287.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1500.pth'\n",
      "Episode: 1510, average reward: 1375.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1510.pth'\n",
      "Episode: 1520, average reward: 1142.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1520.pth'\n",
      "Episode: 1530, average reward: 1144.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1530.pth'\n",
      "Episode: 1540, average reward: 1194.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1540.pth'\n",
      "Episode: 1550, average reward: 1452.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1550.pth'\n",
      "Episode: 1560, average reward: 1430.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1560.pth'\n",
      "Episode: 1570, average reward: 1358.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1570.pth'\n",
      "Episode: 1580, average reward: 1365.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1580.pth'\n",
      "Episode: 1590, average reward: 1261.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1590.pth'\n",
      "Episode: 1600, average reward: 1177.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1600.pth'\n",
      "Episode: 1610, average reward: 1027.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1610.pth'\n",
      "Episode: 1620, average reward: 1279.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1620.pth'\n",
      "Episode: 1630, average reward: 1404.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1630.pth'\n",
      "Episode: 1640, average reward: 1271.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1640.pth'\n",
      "Episode: 1650, average reward: 1288.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1650.pth'\n",
      "Episode: 1660, average reward: 1260.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1660.pth'\n",
      "Episode: 1670, average reward: 1516.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1670.pth'\n",
      "Episode: 1680, average reward: 1205.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1680.pth'\n",
      "Episode: 1690, average reward: 1343.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1690.pth'\n",
      "Episode: 1700, average reward: 1219.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1700.pth'\n",
      "Episode: 1710, average reward: 1252.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1710.pth'\n",
      "Episode: 1720, average reward: 1668.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1720.pth'\n",
      "Episode: 1730, average reward: 1363.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1730.pth'\n",
      "Episode: 1740, average reward: 1482.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1740.pth'\n",
      "Episode: 1750, average reward: 1651.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1750.pth'\n",
      "Episode: 1760, average reward: 1495.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1760.pth'\n",
      "Episode: 1770, average reward: 1385.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1770.pth'\n",
      "Episode: 1780, average reward: 1476.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1780.pth'\n",
      "Episode: 1790, average reward: 1628.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1790.pth'\n",
      "Episode: 1800, average reward: 1758.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1800.pth'\n",
      "Episode: 1810, average reward: 1902.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1810.pth'\n",
      "Episode: 1820, average reward: 1977.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1820.pth'\n",
      "Episode: 1830, average reward: 1908.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1830.pth'\n",
      "Episode: 1840, average reward: 2106.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1840.pth'\n",
      "Episode: 1850, average reward: 2250.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1850.pth'\n",
      "Episode: 1860, average reward: 2035.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1860.pth'\n",
      "Episode: 1870, average reward: 2270.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1870.pth'\n",
      "Episode: 1880, average reward: 2734.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1880.pth'\n",
      "Episode: 1890, average reward: 2787.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1890.pth'\n",
      "Episode: 1900, average reward: 2807.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1900.pth'\n",
      "Episode: 1910, average reward: 2924.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1910.pth'\n",
      "Episode: 1920, average reward: 1942.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1920.pth'\n",
      "Episode: 1930, average reward: 2646.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1930.pth'\n",
      "Episode: 1940, average reward: 1847.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1940.pth'\n",
      "Episode: 1950, average reward: 1977.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1950.pth'\n",
      "Episode: 1960, average reward: 2050.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1960.pth'\n",
      "Episode: 1970, average reward: 2142.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1970.pth'\n",
      "Episode: 1980, average reward: 2013.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1980.pth'\n",
      "Episode: 1990, average reward: 2340.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_1990.pth'\n",
      "Episode: 2000, average reward: 2066.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2000.pth'\n",
      "Episode: 2010, average reward: 1948.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2010.pth'\n",
      "Episode: 2020, average reward: 2295.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2020.pth'\n",
      "Episode: 2030, average reward: 1202.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2030.pth'\n",
      "Episode: 2040, average reward: 1329.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2040.pth'\n",
      "Episode: 2050, average reward: 1922.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2050.pth'\n",
      "Episode: 2060, average reward: 1965.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2060.pth'\n",
      "Episode: 2070, average reward: 2394.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2070.pth'\n",
      "Episode: 2080, average reward: 2773.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2080.pth'\n",
      "Episode: 2090, average reward: 2650.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2090.pth'\n",
      "Episode: 2100, average reward: 2830.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2100.pth'\n",
      "Episode: 2110, average reward: 2897.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2110.pth'\n",
      "Episode: 2120, average reward: 2952.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2120.pth'\n",
      "Episode: 2130, average reward: 2340.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2130.pth'\n",
      "Episode: 2140, average reward: 2483.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2140.pth'\n",
      "Episode: 2150, average reward: 2510.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2150.pth'\n",
      "Episode: 2160, average reward: 2482.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2160.pth'\n",
      "Episode: 2170, average reward: 2219.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2170.pth'\n",
      "Episode: 2180, average reward: 2355.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2180.pth'\n",
      "Episode: 2190, average reward: 896.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2190.pth'\n",
      "Episode: 2200, average reward: 693.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2200.pth'\n",
      "Episode: 2210, average reward: 400.3999938964844\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2210.pth'\n",
      "Episode: 2220, average reward: 841.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2220.pth'\n",
      "Episode: 2230, average reward: 915.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2230.pth'\n",
      "Episode: 2240, average reward: 1117.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2240.pth'\n",
      "Episode: 2250, average reward: 823.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2250.pth'\n",
      "Episode: 2260, average reward: 1244.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2260.pth'\n",
      "Episode: 2270, average reward: 1622.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2270.pth'\n",
      "Episode: 2280, average reward: 1840.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2280.pth'\n",
      "Episode: 2290, average reward: 1726.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2290.pth'\n",
      "Episode: 2300, average reward: 1274.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2300.pth'\n",
      "Episode: 2310, average reward: 1014.2999877929688\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2310.pth'\n",
      "Episode: 2320, average reward: 1307.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2320.pth'\n",
      "Episode: 2330, average reward: 1487.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2330.pth'\n",
      "Episode: 2340, average reward: 1441.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2340.pth'\n",
      "Episode: 2350, average reward: 1659.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2350.pth'\n",
      "Episode: 2360, average reward: 2020.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2360.pth'\n",
      "Episode: 2370, average reward: 1860.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2370.pth'\n",
      "Episode: 2380, average reward: 2214.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2380.pth'\n",
      "Episode: 2390, average reward: 2527.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2390.pth'\n",
      "Episode: 2400, average reward: 2569.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2400.pth'\n",
      "Episode: 2410, average reward: 2013.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2410.pth'\n",
      "Episode: 2420, average reward: 1817.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2420.pth'\n",
      "Episode: 2430, average reward: 1931.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2430.pth'\n",
      "Episode: 2440, average reward: 1968.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2440.pth'\n",
      "Episode: 2450, average reward: 2136.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2450.pth'\n",
      "Episode: 2460, average reward: 2436.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2460.pth'\n",
      "Episode: 2470, average reward: 2476.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2470.pth'\n",
      "Episode: 2480, average reward: 2568.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2480.pth'\n",
      "Episode: 2490, average reward: 2687.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2490.pth'\n",
      "Episode: 2500, average reward: 2621.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2500.pth'\n",
      "Episode: 2510, average reward: 1430.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2510.pth'\n",
      "Episode: 2520, average reward: 1783.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2520.pth'\n",
      "Episode: 2530, average reward: 1273.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2530.pth'\n",
      "Episode: 2540, average reward: 1395.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2540.pth'\n",
      "Episode: 2550, average reward: 2192.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2550.pth'\n",
      "Episode: 2560, average reward: 2764.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2560.pth'\n",
      "Episode: 2570, average reward: 2502.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2570.pth'\n",
      "Episode: 2580, average reward: 2701.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2580.pth'\n",
      "Episode: 2590, average reward: 2804.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2590.pth'\n",
      "Episode: 2600, average reward: 812.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2600.pth'\n",
      "Episode: 2610, average reward: 776.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2610.pth'\n",
      "Episode: 2620, average reward: 1030.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2620.pth'\n",
      "Episode: 2630, average reward: 1271.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2630.pth'\n",
      "Episode: 2640, average reward: 1177.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2640.pth'\n",
      "Episode: 2650, average reward: 1205.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2650.pth'\n",
      "Episode: 2660, average reward: 1426.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2660.pth'\n",
      "Episode: 2670, average reward: 1484.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2670.pth'\n",
      "Episode: 2680, average reward: 1846.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2680.pth'\n",
      "Episode: 2690, average reward: 1691.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2690.pth'\n",
      "Episode: 2700, average reward: 1860.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2700.pth'\n",
      "Episode: 2710, average reward: 1759.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2710.pth'\n",
      "Episode: 2720, average reward: 1732.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2720.pth'\n",
      "Episode: 2730, average reward: 1488.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2730.pth'\n",
      "Episode: 2740, average reward: 1360.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2740.pth'\n",
      "Episode: 2750, average reward: 1507.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2750.pth'\n",
      "Episode: 2760, average reward: 1660.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2760.pth'\n",
      "Episode: 2770, average reward: 1604.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2770.pth'\n",
      "Episode: 2780, average reward: 873.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2780.pth'\n",
      "Episode: 2790, average reward: 873.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2790.pth'\n",
      "Episode: 2800, average reward: 1032.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2800.pth'\n",
      "Episode: 2810, average reward: 1848.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2810.pth'\n",
      "Episode: 2820, average reward: 1952.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2820.pth'\n",
      "Episode: 2830, average reward: 1740.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2830.pth'\n",
      "Episode: 2840, average reward: 1171.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2840.pth'\n",
      "Episode: 2850, average reward: 1663.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2850.pth'\n",
      "Episode: 2860, average reward: 1979.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2860.pth'\n",
      "Episode: 2870, average reward: 1695.0999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2870.pth'\n",
      "Episode: 2880, average reward: 716.2000122070312\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2880.pth'\n",
      "Episode: 2890, average reward: 1857.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2890.pth'\n",
      "Episode: 2900, average reward: 2474.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2900.pth'\n",
      "Episode: 2910, average reward: 2401.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2910.pth'\n",
      "Episode: 2920, average reward: 2061.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2920.pth'\n",
      "Episode: 2930, average reward: 2065.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2930.pth'\n",
      "Episode: 2940, average reward: 2117.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2940.pth'\n",
      "Episode: 2950, average reward: 1818.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2950.pth'\n",
      "Episode: 2960, average reward: 2024.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2960.pth'\n",
      "Episode: 2970, average reward: 2633.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2970.pth'\n",
      "Episode: 2980, average reward: 2637.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2980.pth'\n",
      "Episode: 2990, average reward: 1551.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_2990.pth'\n",
      "Episode: 3000, average reward: 1152.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3000.pth'\n",
      "Episode: 3010, average reward: 1678.0\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3010.pth'\n",
      "Episode: 3020, average reward: 1513.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3020.pth'\n",
      "Episode: 3030, average reward: 1905.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3030.pth'\n",
      "Episode: 3040, average reward: 2838.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3040.pth'\n",
      "Episode: 3050, average reward: 2701.39990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3050.pth'\n",
      "Episode: 3060, average reward: 2731.800048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3060.pth'\n",
      "Episode: 3070, average reward: 2462.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3070.pth'\n",
      "Episode: 3080, average reward: 2283.699951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3080.pth'\n",
      "Episode: 3090, average reward: 1233.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3090.pth'\n",
      "Episode: 3100, average reward: 934.5999755859375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3100.pth'\n",
      "Episode: 3110, average reward: 2498.5\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3110.pth'\n",
      "Episode: 3120, average reward: 2679.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3120.pth'\n",
      "Episode: 3130, average reward: 2636.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3130.pth'\n",
      "Episode: 3140, average reward: 3028.60009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3140.pth'\n",
      "Episode: 3150, average reward: 1591.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3150.pth'\n",
      "Episode: 3160, average reward: 1701.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3160.pth'\n",
      "Episode: 3170, average reward: 2062.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3170.pth'\n",
      "Episode: 3180, average reward: 2751.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3180.pth'\n",
      "Episode: 3190, average reward: 1372.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3190.pth'\n",
      "Episode: 3200, average reward: 2022.4000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3200.pth'\n",
      "Episode: 3210, average reward: 2518.89990234375\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3210.pth'\n",
      "Episode: 3220, average reward: 2577.300048828125\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3220.pth'\n",
      "Episode: 3230, average reward: 2551.199951171875\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3230.pth'\n",
      "Episode: 3240, average reward: 1699.9000244140625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3240.pth'\n",
      "Episode: 3250, average reward: 2123.10009765625\n",
      "Checkpoint saved to 'C:\\Users\\Computing\\Desktop\\PPO without baseline model\\checkpoint_3250.pth'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0093ac6b592a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-b58c0084bf31>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mlog_pis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6486a4657431>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# take the step and record the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;31m# get the reward for this step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver = PPOSolver()\n",
    "while True:\n",
    "    solver.train(solver.sample(), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9751983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4597b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
